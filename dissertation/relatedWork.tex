\chapter{Related Work} 
\label{chap:relWork}

\section{Probabilistic Programming Languages}

With the fast increase in both breadth and specialization of probabilistic models, the need for tools which make the modelling task easier has grown sharply. Many different probabilistic programming languages have arisen to fill this gap. At their core probabilistic programming languages are tools that formalise a model specification, by providing the user with stochastic primitives with which to define their model. Given this formalisation the PPL can then attempt to automatically perform inference on the user's model. While the foundation for these stochastic languages was set a long time ago (eg: \cite{jones1989probabilistic}), there has been a recent explosion in the popularity of PPLs. Probabilistic languages have now arisen to fill many different modelling niches by juggling tradeoffs such as efficiency, expressivity and ease of use. 

It is beyond the scope of this work to go into details about all the various PPLs (see \cite{ppw} for a short summary of some of the most well known PPLs), but we shall briefly list a few different paradigms, just to give an idea of the diversity of PPLs. The logic programming paradigm has atracted quite a few probabilistic extensions (\cite{kimmig2011implementation, de2008probabilistic, sato1997prism, poole2008independent}) which are used for tasks such as symbolic-statistical modeling and statistical relational learning. Other PPLs adopt a functional programming paradigm (\cite{goodman2008church, mansinghka2014venture, wood2014new}), by extending LISP \cite{abelson1991revised} which, being a higher-order language, gives the user a large degree of flexibility over his probabilistic program specification. Some methods for creating embedded, domain specific, PPLs have also emerged \cite{kiselyov2009embedded}. Finally, BUGS is an example of a PPL that does not extend a previous programming language, but instead defines a simple, bespoke, language that emphasizes usability, since it enables the user to define their model in a declarative fashion \cite{lunn2009bugs}.

Switching to another criteria, some PPLs focus on efficiency at the cost of expressivity, generally by restricting themselves to programs that can be compiled into finite graphical models \cite{lunn2009bugs, richardson2006markov, mccallum2009factorie, stan-software:2014, minkainfer}. Other PPLs emphasize flexibility and expresiveness (\cite{milch20071, pfeffer2001ibal, pfeffer2009figaro, goodman2008church}), by allowing the user to specify programs that cannot be expressed as graphical models and that therefore can't make use of the standard graphical model inference methods. Church and Venture are the two languages that pehaps focus most on flexibility from the above, which is what makes them well suited for complex, potentially recursive, reasoning tasks like those discussed in Section \ref{sect:betInf}.

It is based on these criteria that we decided to compare two popular PPL that are as different from one another as possible. Namely the efficient, declarative, OpenBUGS and the flexible, functional Venture.



One possibility is to perform exact inference by applying dynamic programming (DP) techniques to manage the exponential number of possible execution paths \cite{stuhlmuller2012dynamic}. There is potential for future work in this area by analysing the performance of different DP and approximate-DP techniques. More generally, it is clear that DP wonâ€™t work on all models, but understanding what the subclass
of models is on which exact inference might be tractable remains an open problem. 

When exact inference is intractable, we have to settle for approximate solutions, usually obtained via Markov chain Monte Carlo (MCMC). Figuring out how to best take advantage of the underlying structure
of the distributions, as to obtain better mixing rates and therefore faster inference, is currently an area of active research. One attempt uses nonstandard interpretations to create monad-like side computations which can extract structural information, such as gradients \cite{wingate2011nonstandard}. This information can then enable the use of sophisticated MCMC techniques, such as Hamiltonian MC, which can lead to big boosts in performance over more naive MCMC methods. There is much potential for future work in applying further compiler design and program analysis techniques towards speeding up inference. For instance, speed-ups of over an order of magnitude were shown to be possible by applying techniques such as JIT compilation, dead code elimination, allocation removal and incremental optimization \cite{yang2013incrementalizing}.

