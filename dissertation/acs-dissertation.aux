\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{ppaml}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{brf}{\backcite{ppaml}{{1}{1}{chapter.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Importance}{1}{section.1.1}}
\newlabel{sect:importance}{{1.1}{1}{Importance\relax }{section.1.1}{}}
\citation{goodman2013principles}
\citation{wingate2011nonstandard}
\citation{wood2014new}
\citation{stuhlmuller2012dynamic}
\citation{yeh2012synthesizing}
\citation{mansinghka2014venture}
\citation{yang2013incrementalizing}
\citation{goodman2013knowledge}
\citation{frank2012predicting}
\citation{stuhlmuller2012dynamic}
\citation{wingate2011nonstandard}
\citation{yang2013incrementalizing}
\@writefile{brf}{\backcite{goodman2013principles, wingate2011nonstandard, wood2014new}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{stuhlmuller2012dynamic, yeh2012synthesizing}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{mansinghka2014venture, yang2013incrementalizing}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{goodman2013knowledge, frank2012predicting}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{stuhlmuller2012dynamic}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{wingate2011nonstandard}{{3}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{yang2013incrementalizing}{{3}{1.1}{section.1.1}}}
\citation{bugsOverview}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Comparing Venture and OpenBUGS}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{5}{section.2.1}}
\citation{lunn2009bugs}
\citation{mansinghka2014venture}
\citation{roy2008stochastic}
\citation{probMods}
\citation{gerstenberg2012ping}
\citation{mansinghka2009natively}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Preliminaries}{6}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}OpenBUGS}{6}{subsection.2.2.1}}
\@writefile{brf}{\backcite{bugsOverview}{{6}{2.2.1}{subsection.2.2.1}}}
\@writefile{brf}{\backcite{lunn2009bugs}{{6}{2.2.1}{subsection.2.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Venture}{6}{subsection.2.2.2}}
\@writefile{brf}{\backcite{mansinghka2014venture}{{6}{2.2.2}{subsection.2.2.2}}}
\@writefile{brf}{\backcite{roy2008stochastic}{{6}{2.2.2}{subsection.2.2.2}}}
\@writefile{brf}{\backcite{probMods, gerstenberg2012ping}{{7}{2.2.2}{subsection.2.2.2}}}
\@writefile{brf}{\backcite{mansinghka2009natively}{{7}{2.2.2}{subsection.2.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Number of MCMC steps}{7}{subsection.2.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Strategies for extracting S samples from a model with V unconditioned variables\relax }}{7}{table.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:noSteps}{{2.1}{7}{Strategies for extracting S samples from a model with V unconditioned variables\relax \relax }{table.caption.5}{}}
\citation{TdfBugsRepo}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Empirical results}{8}{section.2.3}}
\@writefile{tdo}{\contentsline {todo}{{Maybe also evaluate one model that's not from the OpenBUGS repository, since BUGS might be unreasonably optimized on its own models.}}{8}{section*.6}}
\pgfsyspdfmark {pgfid1}{12627250}{42602286}
\pgfsyspdfmark {pgfid2}{5539101}{43642941}
\pgfsyspdfmark {pgfid3}{9159964}{40248645}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Tdf model description}{8}{subsection.2.3.1}}
\newlabel{sect:tdfDesc}{{2.3.1}{8}{Tdf model description\relax }{subsection.2.3.1}{}}
\@writefile{brf}{\backcite{TdfBugsRepo}{{8}{2.3.1}{subsection.2.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Tdf model true posteriors}{8}{subsection.2.3.2}}
\newlabel{sect:truePost}{{2.3.2}{8}{Tdf model true posteriors\relax }{subsection.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces True posterior distributions for the 3 Tdf models.\relax }}{9}{figure.caption.7}}
\newlabel{fig:tdfPosts}{{2.1}{9}{True posterior distributions for the 3 Tdf models.\relax \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Tdf model results}{9}{subsection.2.3.3}}
\@writefile{tdo}{\contentsline {todo}{{Maybe do more runs and plot quartiles}}{9}{section*.9}}
\pgfsyspdfmark {pgfid6}{6571294}{8126807}
\pgfsyspdfmark {pgfid9}{33522973}{8909258}
\pgfsyspdfmark {pgfid10}{37143836}{7915232}
\newlabel{fig:tdfCourseDiscSamp}{{2.2a}{10}{Distributions obtained by the two PPLs on the course discrete model.\relax \relax }{figure.caption.8}{}}
\newlabel{sub@fig:tdfCourseDiscSamp}{{a}{10}{Distributions obtained by the two PPLs on the course discrete model.\relax \relax }{figure.caption.8}{}}
\newlabel{fig:tdfCourseDiscConv}{{2.2b}{10}{Rate of convergence of the two engines as the number of samples increases (first 1,000 samples are discarded as burn-in). The black line shows the KL divergence achieved by Venture after 101,000 inference steps.\relax \relax }{figure.caption.8}{}}
\newlabel{sub@fig:tdfCourseDiscConv}{{b}{10}{Rate of convergence of the two engines as the number of samples increases (first 1,000 samples are discarded as burn-in). The black line shows the KL divergence achieved by Venture after 101,000 inference steps.\relax \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Performance of the Tdf course discrete model\relax }}{10}{figure.caption.8}}
\citation{lunn2000winbugs}
\citation{keith2008generalized}
\newlabel{fig:tdfFineDiscSamp}{{2.3a}{11}{Distributions obtained by the two PPLs on the fine discrete model.\relax \relax }{figure.caption.10}{}}
\newlabel{sub@fig:tdfFineDiscSamp}{{a}{11}{Distributions obtained by the two PPLs on the fine discrete model.\relax \relax }{figure.caption.10}{}}
\newlabel{fig:tdfFineDiscConv}{{2.3b}{11}{Rate of convergence of the two engines as the number of samples increases (first 1,000 samples are discarded as burn-in). The black line shows the KL divergence achieved by Venture after 101,000 inference steps.\relax \relax }{figure.caption.10}{}}
\newlabel{sub@fig:tdfFineDiscConv}{{b}{11}{Rate of convergence of the two engines as the number of samples increases (first 1,000 samples are discarded as burn-in). The black line shows the KL divergence achieved by Venture after 101,000 inference steps.\relax \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Performance of the Tdf fine discrete model\relax }}{11}{figure.caption.10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Analysis of Venture's performance}{11}{section.2.4}}
\@writefile{brf}{\backcite{lunn2000winbugs, keith2008generalized}{{11}{2.4}{figure.caption.12}}}
\@writefile{tdo}{\contentsline {todo}{{Decide is there's anything worth reporting from the performance tests done on Venture. Log dates: 2014.02.19 and 2014.02.20}}{11}{section*.13}}
\citation{webChurch}
\newlabel{fig:tdfContSamp}{{2.4a}{12}{Distributions obtained by the two PPLs on the continuous model.\relax \relax }{figure.caption.11}{}}
\newlabel{sub@fig:tdfContSamp}{{a}{12}{Distributions obtained by the two PPLs on the continuous model.\relax \relax }{figure.caption.11}{}}
\newlabel{fig:tdfContConv}{{2.4b}{12}{Rate of convergence of the two engines as the number of samples increases (first 1,000 samples are discarded as burn-in). The black line shows the KS difference achieved by Venture after 101,000 inference steps.\relax \relax }{figure.caption.11}{}}
\newlabel{sub@fig:tdfContConv}{{b}{12}{Rate of convergence of the two engines as the number of samples increases (first 1,000 samples are discarded as burn-in). The black line shows the KS difference achieved by Venture after 101,000 inference steps.\relax \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Performance of the Tdf continuous model\relax }}{12}{figure.caption.11}}
\pgfsyspdfmark {pgfid11}{9913630}{31009765}
\pgfsyspdfmark {pgfid12}{5539101}{28550774}
\pgfsyspdfmark {pgfid13}{9159964}{25647998}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Webchurch's performance}{12}{subsection.2.4.1}}
\@writefile{brf}{\backcite{webChurch}{{12}{2.4.1}{subsection.2.4.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Number and size of identical sample runs generated by the two PPLs on the continuous model.\relax }}{13}{figure.caption.12}}
\newlabel{fig:tdfContRun}{{2.5}{13}{Number and size of identical sample runs generated by the two PPLs on the continuous model.\relax \relax }{figure.caption.12}{}}
\citation{kullback1951information}
\citation{massey1951kolmogorov}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Partitioned Priors}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{{say something about proposal kernels vs. sampling from prior?}}{15}{section*.14}}
\pgfsyspdfmark {pgfid16}{22655715}{28754416}
\pgfsyspdfmark {pgfid19}{33522973}{30028387}
\pgfsyspdfmark {pgfid20}{37143836}{28542841}
\@writefile{tdo}{\contentsline {todo}{{not sure if this makes much sense. Need to research it or remove}}{15}{section*.15}}
\pgfsyspdfmark {pgfid21}{22024914}{19481123}
\pgfsyspdfmark {pgfid24}{33522973}{20697764}
\pgfsyspdfmark {pgfid25}{37143836}{19269548}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Preliminaries}{15}{section.3.1}}
\citation{metropolis1953equation}
\citation{hastings1970monte}
\citation{neal1993probabilistic}
\@writefile{brf}{\backcite{kullback1951information}{{16}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{massey1951kolmogorov}{{16}{3.1}{section.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Metropolis-Hastings}{16}{subsection.3.1.1}}
\newlabel{sect:MH}{{3.1.1}{16}{Metropolis-Hastings\relax }{subsection.3.1.1}{}}
\@writefile{brf}{\backcite{metropolis1953equation, hastings1970monte}{{16}{3.1.1}{subsection.3.1.1}}}
\@writefile{brf}{\backcite{neal1993probabilistic}{{16}{3.1.1}{subsection.3.1.1}}}
\citation{mackay2003information}
\citation{neal1993probabilistic}
\@writefile{brf}{\backcite{mackay2003information, neal1993probabilistic}{{17}{3.1.1}{subsection.3.1.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Expected number of iterations to a neighbourhood of the mode}{17}{subsection.3.1.2}}
\newlabel{section:sampsToMode}{{3.1.2}{17}{Expected number of iterations to a neighbourhood of the mode\relax }{subsection.3.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Mixing properties around the mode}{18}{subsection.3.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Sum of uniforms}{18}{section.3.2}}
\newlabel{sect:sumUnif}{{3.2}{18}{Sum of uniforms\relax }{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Distribution induced by partitioning the prior (uniform-continuous 2 100) into (uniform-continuous 2 95) + (uniform-continuous 0 2) (uniform-continuous 0 1).\relax }}{19}{figure.caption.16}}
\newlabel{fig:1295Prior}{{3.1}{19}{Distribution induced by partitioning the prior (uniform-continuous 2 100) into (uniform-continuous 2 95) + (uniform-continuous 0 2) (uniform-continuous 0 1).\relax \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Finding a good sum decomposition}{19}{subsection.3.2.1}}
\newlabel{sect:goodSum}{{3.2.1}{19}{Finding a good sum decomposition\relax }{subsection.3.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Expected number of steps to mode neighbourhoods on the Tdf continuous model for an unpartitioned prior and some of the best sum decomposition priors.\relax }}{20}{table.caption.17}}
\newlabel{tab:bestParts}{{3.1}{20}{Expected number of steps to mode neighbourhoods on the Tdf continuous model for an unpartitioned prior and some of the best sum decomposition priors.\relax \relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Average distance travelled around the mode on the Tdf continuous model for an unpartitioned prior and some of the best sum decomposition priors.\relax }}{21}{table.caption.18}}
\newlabel{tab:partMix}{{3.2}{21}{Average distance travelled around the mode on the Tdf continuous model for an unpartitioned prior and some of the best sum decomposition priors.\relax \relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Evaluating the (1,2,95) sum decomposition}{21}{subsection.3.2.2}}
\newlabel{sect:1295Eval}{{3.2.2}{21}{Evaluating the (1,2,95) sum decomposition\relax }{subsection.3.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sample evolutions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf continuous model.\relax }}{21}{figure.caption.19}}
\newlabel{fig:tdfPSampEvol}{{3.2}{21}{Sample evolutions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf continuous model.\relax \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sample autocorrelations for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf continuous model.\relax }}{22}{figure.caption.20}}
\newlabel{fig:tdfPAutoCorr}{{3.3}{22}{Sample autocorrelations for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf continuous model.\relax \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Sample distributions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf continuous model.\relax }}{22}{figure.caption.21}}
\newlabel{fig:tdfPDist}{{3.4}{22}{Sample distributions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf continuous model.\relax \relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Tdf21 model}{22}{section*.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The true posteriors of the Tdf continuous and the Tdf21 continuous models.\relax }}{23}{figure.caption.23}}
\newlabel{fig:tdfPPost}{{3.5}{23}{The true posteriors of the Tdf continuous and the Tdf21 continuous models.\relax \relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The true log-likelihoods of the Tdf continuous and the Tdf21 continuous models.\relax }}{23}{figure.caption.24}}
\newlabel{fig:tdfPLL}{{3.6}{23}{The true log-likelihoods of the Tdf continuous and the Tdf21 continuous models.\relax \relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluating the decomposition on the Tdf21 model}{23}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Sample evolutions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf21 model.\relax }}{24}{figure.caption.26}}
\newlabel{fig:tdf21PSampEvol}{{3.7}{24}{Sample evolutions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf21 model.\relax \relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Sample autocorrelations for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf21 model.\relax }}{24}{figure.caption.27}}
\newlabel{fig:tdf21PAutoCorr}{{3.8}{24}{Sample autocorrelations for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf21 model.\relax \relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Bit decomposition}{24}{section.3.3}}
\newlabel{sect:bitDecomp}{{3.3}{24}{Bit decomposition\relax }{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Sample distributions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf21 model.\relax }}{25}{figure.caption.28}}
\newlabel{fig:tdf21PSampDist}{{3.9}{25}{Sample distributions for the unpartitioned and the (1, 2, 95) partitioned priors, over 10,000 samples on the Tdf21 model.\relax \relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Definition}{25}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Evaluation on Tdf and Tdf21}{25}{subsection.3.3.2}}
\@writefile{tdo}{\contentsline {todo}{{Should be possible to give some more formal results here.}}{26}{section*.29}}
\pgfsyspdfmark {pgfid26}{26461371}{46661975}
\pgfsyspdfmark {pgfid27}{5539101}{45677544}
\pgfsyspdfmark {pgfid28}{9159964}{44249328}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Sample distributions for an unpartitioned and a 3 bit decomposition prior, over 10,000 samples on the Tdf continuous model.\relax }}{26}{figure.caption.30}}
\newlabel{fig:tdfPSampDist}{{3.10}{26}{Sample distributions for an unpartitioned and a 3 bit decomposition prior, over 10,000 samples on the Tdf continuous model.\relax \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Sample distributions for an unpartitioned and a 3 bit decomposition prios, over 10,000 samples on the Tdf21 model.\relax }}{26}{figure.caption.31}}
\newlabel{fig:tdf21PSampDist}{{3.11}{26}{Sample distributions for an unpartitioned and a 3 bit decomposition prios, over 10,000 samples on the Tdf21 model.\relax \relax }{figure.caption.31}{}}
\@writefile{tdo}{\contentsline {todo}{{talk about how binomials of different depth perform here}}{26}{section*.32}}
\pgfsyspdfmark {pgfid31}{11813820}{11075927}
\pgfsyspdfmark {pgfid32}{5539101}{12349898}
\pgfsyspdfmark {pgfid33}{9159964}{10864352}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Expected number of steps to neighbourhoods of the mode on the Tdf and Tdf21 continuous models for an unpartitioned prior, the (1,2,95) sum decomposition and 2 bit decompositions.\relax }}{27}{table.caption.33}}
\newlabel{tab:bestParts}{{3.3}{27}{Expected number of steps to neighbourhoods of the mode on the Tdf and Tdf21 continuous models for an unpartitioned prior, the (1,2,95) sum decomposition and 2 bit decompositions.\relax \relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Getting stuck on a bad sample}{27}{subsection.3.3.3}}
\newlabel{sect:stuckSamples}{{3.3.3}{27}{Getting stuck on a bad sample\relax }{subsection.3.3.3}{}}
\citation{mackay2003information}
\@writefile{tdo}{\contentsline {todo}{{is a more thorough analysis feasible here?}}{28}{section*.34}}
\pgfsyspdfmark {pgfid36}{15763512}{31850839}
\pgfsyspdfmark {pgfid37}{5539101}{32633290}
\pgfsyspdfmark {pgfid38}{9159964}{31639264}
\@writefile{tdo}{\contentsline {todo}{{add more analysis on this}}{28}{section*.35}}
\pgfsyspdfmark {pgfid41}{31781332}{19988823}
\pgfsyspdfmark {pgfid42}{5539101}{20713944}
\pgfsyspdfmark {pgfid43}{9159964}{19777248}
\@writefile{brf}{\backcite{mackay2003information}{{29}{3.3.3}{section*.35}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Mixtures of shifted bit decompositions}{29}{subsection.3.3.4}}
\newlabel{sect:shifts}{{3.3.4}{29}{Mixtures of shifted bit decompositions\relax }{subsection.3.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Avoiding getting stuck}{29}{section*.36}}
\newlabel{sect:stuckMath}{{3.3.4}{29}{Avoiding getting stuck\relax }{section*.36}{}}
\@writefile{tdo}{\contentsline {todo}{{not sure if this section is clear enough. add diagrams?}}{29}{section*.37}}
\pgfsyspdfmark {pgfid46}{6571294}{27213957}
\pgfsyspdfmark {pgfid49}{33522973}{28487928}
\pgfsyspdfmark {pgfid50}{37143836}{27002382}
\@writefile{toc}{\contentsline {subsubsection}{Empirical performance}{32}{section*.38}}
\newlabel{fig:allMaxShifts}{{3.12a}{32}{Maximum sized shifts.\relax \relax }{figure.caption.39}{}}
\newlabel{sub@fig:allMaxShifts}{{a}{32}{Maximum sized shifts.\relax \relax }{figure.caption.39}{}}
\newlabel{fig:allMinShifts}{{3.12b}{32}{Minimum sized shifts.\relax \relax }{figure.caption.39}{}}
\newlabel{sub@fig:allMinShifts}{{b}{32}{Minimum sized shifts.\relax \relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Time to 0.001 neighbourhood of mode, averaged over all mode placements, for bit decompositions of different depths using a shift for every bit.\relax }}{32}{figure.caption.39}}
\newlabel{fig:allShifts}{{3.12}{32}{Time to 0.001 neighbourhood of mode, averaged over all mode placements, for bit decompositions of different depths using a shift for every bit.\relax \relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Time to 0.01 neighbourhood of mode, averaged over all mode placements, for bit decompositions of different depths using a shift for every bit.\relax }}{33}{figure.caption.40}}
\newlabel{fig:AllShiftsMax001}{{3.13}{33}{Time to 0.01 neighbourhood of mode, averaged over all mode placements, for bit decompositions of different depths using a shift for every bit.\relax \relax }{figure.caption.40}{}}
\@writefile{tdo}{\contentsline {todo}{{talk about distributions over shifts, and more about benefits/drawback of shifts and/or changing depths}}{33}{section*.41}}
\pgfsyspdfmark {pgfid51}{6571294}{20513210}
\pgfsyspdfmark {pgfid54}{33522973}{22770221}
\pgfsyspdfmark {pgfid55}{37143836}{20301635}
\@writefile{toc}{\contentsline {subsubsection}{Determining optimal shifts and shift transitions}{33}{section*.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Time to 0.01 neighbourhood of mode, averaged over all mode placements, for bit decompositions of different depth using a small number of shifts.\relax }}{34}{figure.caption.42}}
\newlabel{fig:fewShifts}{{3.14}{34}{Time to 0.01 neighbourhood of mode, averaged over all mode placements, for bit decompositions of different depth using a small number of shifts.\relax \relax }{figure.caption.42}{}}
\@writefile{tdo}{\contentsline {todo}{{There seem to be heuristic ways to address this}}{34}{section*.44}}
\pgfsyspdfmark {pgfid56}{11552032}{15150053}
\pgfsyspdfmark {pgfid57}{5539101}{15932504}
\pgfsyspdfmark {pgfid58}{9159964}{14938478}
\@writefile{tdo}{\contentsline {todo}{{try to get some results using markov chain implementation. Otherwise there's not much point in describing it ...}}{34}{section*.45}}
\pgfsyspdfmark {pgfid61}{9913630}{9448421}
\pgfsyspdfmark {pgfid62}{5539101}{11705432}
\pgfsyspdfmark {pgfid63}{9159964}{9236846}
\citation{neal2003slice}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Novel PPL inference techniques}{35}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:infEngines}{{4}{35}{Novel PPL inference techniques\relax }{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Preliminaries}{35}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Slice sampling}{35}{subsection.4.1.1}}
\newlabel{sect:sliceBack}{{4.1.1}{35}{Slice sampling\relax }{subsection.4.1.1}{}}
\@writefile{brf}{\backcite{neal2003slice}{{35}{4.1.1}{subsection.4.1.1}}}
\citation{neal2003slice}
\citation{mackay2003information}
\citation{wingate2011lightweight}
\citation{wingate2011lightweight}
\citation{green2009reversible}
\citation{stocPy}
\@writefile{brf}{\backcite{neal2003slice, mackay2003information}{{37}{4.1.1}{subsection.4.1.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Basic PPL Construction}{37}{subsection.4.1.2}}
\newlabel{pplBack}{{4.1.2}{37}{Basic PPL Construction\relax }{subsection.4.1.2}{}}
\@writefile{brf}{\backcite{wingate2011lightweight}{{37}{4.1.2}{subsection.4.1.2}}}
\@writefile{brf}{\backcite{wingate2011lightweight}{{37}{4.1.2}{subsection.4.1.2}}}
\@writefile{brf}{\backcite{green2009reversible}{{37}{4.1.2}{subsection.4.1.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Stochastic Python}{37}{section.4.2}}
\newlabel{sect:StocPy}{{4.2}{37}{Stochastic Python\relax }{section.4.2}{}}
\@writefile{brf}{\backcite{stocPy}{{37}{4.2}{section.4.2}}}
\citation{wingate2011lightweight}
\@writefile{brf}{\backcite{wingate2011lightweight}{{38}{4.2}{section.4.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Slice sampling inference engine}{38}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Custom Slice Sampling and Metropolis on Tdf models}{38}{subsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Burn-in time for local metropolis-hastings and slice sampling, on the two continuous Tdf models, as the target neighbourhood varies.\relax }}{39}{figure.caption.46}}
\newlabel{fig:SliceMetCustPerf}{{4.1}{39}{Burn-in time for local metropolis-hastings and slice sampling, on the two continuous Tdf models, as the target neighbourhood varies.\relax \relax }{figure.caption.46}{}}
\citation{wingate2011lightweight}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Average burn-in time for slice sampling guided by a Gaussian likelihood of varying mean and standard deviation\relax }}{40}{figure.caption.47}}
\newlabel{fig:sliceGaussLik}{{4.2}{40}{Average burn-in time for slice sampling guided by a Gaussian likelihood of varying mean and standard deviation\relax \relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The smallest and largest Gaussian standard deviation considered in Figure \ref  {fig:sliceGaussLik}\relax }}{40}{figure.caption.48}}
\newlabel{fig:gaussStdDev}{{4.3}{40}{The smallest and largest Gaussian standard deviation considered in Figure \ref {fig:sliceGaussLik}\relax \relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Empirical sample distribution of local metropolis and slice sampling on the Tdf continuous model\relax }}{40}{figure.caption.51}}
\newlabel{fig:custSampDist}{{4.6}{40}{Empirical sample distribution of local metropolis and slice sampling on the Tdf continuous model\relax \relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Sample evolution of local metropolis and slice sampling on the Tdf continuous model\relax }}{41}{figure.caption.49}}
\newlabel{fig:custSampEvol}{{4.4}{41}{Sample evolution of local metropolis and slice sampling on the Tdf continuous model\relax \relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Sample autocorrelation of local metropolis and slice sampling on the Tdf continuous model\relax }}{41}{figure.caption.50}}
\newlabel{fig:custAutoCorr}{{4.5}{41}{Sample autocorrelation of local metropolis and slice sampling on the Tdf continuous model\relax \relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Generic, lightweight, slice sampling inference engine}{41}{subsection.4.3.2}}
\@writefile{brf}{\backcite{wingate2011lightweight}{{41}{4.3.2}{subsection.4.3.2}}}
\@writefile{toc}{\contentsline {subsubsection}{Slice sampling on the Tdf model}{42}{section*.52}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Sample distribution from running Venture and stochastic python versions of metropolis and slice sampling for 10 minutes on the Tdf continuous model.\relax }}{42}{figure.caption.53}}
\newlabel{fig:tdfSampDists}{{4.7}{42}{Sample distribution from running Venture and stochastic python versions of metropolis and slice sampling for 10 minutes on the Tdf continuous model.\relax \relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Comparison of Kolmogorov-Smirnov differences between true and inferred posteriors.\relax }}{43}{figure.caption.54}}
\newlabel{fig:TdfSliceLIComp}{{4.8}{43}{Comparison of Kolmogorov-Smirnov differences between true and inferred posteriors.\relax \relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{Slice sampling on gaussian mean inference models}{43}{section*.55}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Analytically derived posteriors of the NormalMean1, NormalMean2 and NormalMean3 models.\relax }}{44}{figure.caption.56}}
\newlabel{fig:tdfSampDists}{{4.9}{44}{Analytically derived posteriors of the NormalMean1, NormalMean2 and NormalMean3 models.\relax \relax }{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the 1 dimensional NormalMean1 model.\relax }}{44}{figure.caption.57}}
\newlabel{fig:normal1Perf}{{4.10}{44}{Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the 1 dimensional NormalMean1 model.\relax \relax }{figure.caption.57}{}}
\citation{wood2014new}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the 2 dimensional NormalMean2 model.\relax }}{45}{figure.caption.58}}
\newlabel{fig:normal2Perf}{{4.11}{45}{Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the 2 dimensional NormalMean2 model.\relax \relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the trans-dimensional NormalMean3 model.\relax }}{45}{figure.caption.59}}
\newlabel{fig:normal4Perf}{{4.12}{45}{Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the trans-dimensional NormalMean3 model.\relax \relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsubsection}{Branching Model}{46}{section*.60}}
\newlabel{sect:branching}{{4.3.2}{46}{Branching Model\relax }{section*.60}{}}
\@writefile{brf}{\backcite{wood2014new}{{46}{4.3.2}{section*.60}}}
\@writefile{tdo}{\contentsline {todo}{{mention the discrepancy with the paper?}}{46}{section*.61}}
\pgfsyspdfmark {pgfid66}{9913630}{27919936}
\pgfsyspdfmark {pgfid67}{5539101}{28702387}
\pgfsyspdfmark {pgfid68}{9159964}{27708361}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces True posterior for the Branching Model\relax }}{46}{figure.caption.62}}
\newlabel{fig:BranchPost}{{4.13}{46}{True posterior for the Branching Model\relax \relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the Branching model.\relax }}{47}{figure.caption.63}}
\newlabel{fig:branchPerf}{{4.14}{47}{Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the Branching model.\relax \relax }{figure.caption.63}{}}
\@writefile{tdo}{\contentsline {todo}{{talk about the continuous vs. discrete aspect and the domain in which we expect slice to be good}}{47}{section*.64}}
\pgfsyspdfmark {pgfid71}{25768654}{11536495}
\pgfsyspdfmark {pgfid74}{33522973}{13793506}
\pgfsyspdfmark {pgfid75}{37143836}{11324920}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the Branching model.\relax }}{48}{figure.caption.65}}
\newlabel{fig:branchPerfSamps}{{4.15}{48}{Runs and quartiles generated by slice, metropolis and mixtures of metropolis and slice on the Branching model.\relax \relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsubsection}{Trans-dimensional slice sampling}{48}{section*.66}}
\newlabel{sect:tdSlice}{{4.3.2}{48}{Trans-dimensional slice sampling\relax }{section*.66}{}}
\newlabel{fig:branchTraceLik}{{4.16a}{49}{Space of trace likelihoods if both variables are always sampled.\relax \relax }{figure.caption.67}{}}
\newlabel{sub@fig:branchTraceLik}{{a}{49}{Space of trace likelihoods if both variables are always sampled.\relax \relax }{figure.caption.67}{}}
\newlabel{fig:branchPost}{{4.16b}{49}{True posterior of Branching model.\relax \relax }{figure.caption.67}{}}
\newlabel{sub@fig:branchPost}{{b}{49}{True posterior of Branching model.\relax \relax }{figure.caption.67}{}}
\@writefile{tdo}{\contentsline {todo}{{mention buggy metropolis version that also samples from this}}{49}{section*.68}}
\pgfsyspdfmark {pgfid76}{8338514}{25428311}
\pgfsyspdfmark {pgfid79}{33522973}{26702282}
\pgfsyspdfmark {pgfid80}{37143836}{25216736}
\newlabel{fig:branchWrongTraceLik}{{4.16c}{49}{Space of trace likelihoods if both variables are always sampled.\relax \relax }{figure.caption.69}{}}
\newlabel{sub@fig:branchWrongTraceLik}{{c}{49}{Space of trace likelihoods if both variables are always sampled.\relax \relax }{figure.caption.69}{}}
\newlabel{fig:branchWrongPost}{{4.16d}{49}{Space of trace likelihoods implied by naive slice sampling.\relax \relax }{figure.caption.69}{}}
\newlabel{sub@fig:branchWrongPost}{{d}{49}{Space of trace likelihoods implied by naive slice sampling.\relax \relax }{figure.caption.69}{}}
\citation{robert2004monte}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Runs and quartiles generated by metropolis, a mixture of metropolis and slice, and both the modified and the naive slice algorithms on the NormalMean3 model.\relax }}{50}{figure.caption.70}}
\newlabel{fig:normal4TD}{{4.16}{50}{Runs and quartiles generated by metropolis, a mixture of metropolis and slice, and both the modified and the naive slice algorithms on the NormalMean3 model.\relax \relax }{figure.caption.70}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Quasi-Monte Carlo}{50}{section.4.4}}
\@writefile{brf}{\backcite{robert2004monte}{{50}{4.4}{section.4.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Runs and quartiles generated by metropolis, a mixture of metropolis and slice, and both the corrected slice and the naive slice algorithms on the Branching model.\relax }}{51}{figure.caption.71}}
\newlabel{fig:branchTD}{{4.17}{51}{Runs and quartiles generated by metropolis, a mixture of metropolis and slice, and both the corrected slice and the naive slice algorithms on the Branching model.\relax \relax }{figure.caption.71}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Related Work}{53}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{dissertation}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Summary and Conclusions}{55}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{ppaml}{1}
\bibcite{frank2012predicting}{2}
\bibcite{gerstenberg2012ping}{3}
\bibcite{probMods}{4}
\bibcite{webChurch}{5}
\bibcite{goodman2013principles}{6}
\bibcite{goodman2013knowledge}{7}
\bibcite{green2009reversible}{8}
\bibcite{hastings1970monte}{9}
\bibcite{keith2008generalized}{10}
\bibcite{kullback1951information}{11}
\bibcite{lunn2009bugs}{12}
\bibcite{lunn2000winbugs}{13}
\bibcite{mackay2003information}{14}
\bibcite{mansinghka2014venture}{15}
\bibcite{mansinghka2009natively}{16}
\bibcite{massey1951kolmogorov}{17}
\bibcite{metropolis1953equation}{18}
\bibcite{neal1993probabilistic}{19}
\bibcite{neal2003slice}{20}
\bibcite{bugsOverview}{21}
\bibcite{stocPy}{22}
\bibcite{TdfBugsRepo}{23}
\bibcite{robert2004monte}{24}
\bibcite{roy2008stochastic}{25}
\bibcite{stuhlmuller2012dynamic}{26}
\bibcite{wingate2011nonstandard}{27}
\bibcite{wingate2011lightweight}{28}
\bibcite{wood2014new}{29}
\bibcite{yang2013incrementalizing}{30}
\bibcite{yeh2012synthesizing}{31}
