\chapter{Introduction}

\pagenumbering{arabic} 
\setcounter{page}{1} 

Probabilistic programming languages (PPLs) have garnered a lot of attention recently, especially since the announcement of a DARPA\footnote{Defense Advanced Research Projects Agency} initiative to support their research and development \cite{ppaml}. However, a lot of progress has to be made before the promise of PPLs can be reached, with one of the critical areas of current research being the inference engines used to convert programs in these languages into statistical inference algorithms. This project aims to better understand the complex interactions and performance trade-offs inherent between probabilistic programming languages, model types and inference methods. By better understanding these interactions we hope to discover areas where improvements are needed and explore ways in which such improvements could be carried out.

\section{Importance of Probabilistic Programming Languages}
\label{sect:importance}
Machine learning has become ubiquitous, with applications ranging from self-driving cars to gene sequencing. However, building any significant machine learning application currently involves a great deal of expertise in both defining an adequate statistical model and implementing inference algorithms for this model in order to extract useful information from your data. These challenges create a significant bottleneck towards wide-range adoption of machine learning solutions.

Probabilistic programming attempts to alleviate this problem by letting the user describe their model and statistical queries in a high level programming language. Such a language would provide convenient methods of describing complex probability distributions, and have an inference engine capable of automatically generating the necessary statistical inference code. This higher-level approach to modelling aims to duplicate some of the benefits gained by the switch from assembly to higher-level programming languages, namely allowing a wider range of developers to work on more complex problems with less effort and at a lower cost. If successful, these techniques could not only make probabilistic modelling cheap and simple enough to become ubiquitous, but also enable the construction of applications that are inconceivable today.

\section{Need for better inference}
\label{sect:betInf}
A lot of current research in probabilistic programming is focused on achieving more efficient automatic inference on different types of models (eg: \cite{goodman2013principles, wingate2011nonstandard, wood2014new}). This problem has been approached from many angles, ranging from the development of specialized inference methods that work well on certain, restricted, classes of models (eg: \cite{stuhlmuller2012dynamic, yeh2012synthesizing}), to employing general inference techniques on models transformed by the application of optimization techniques similar to those used in compilers (eg: \cite{mansinghka2014venture, yang2013incrementalizing}).

If we manage to surpass the hurdle of constructing generic inference engines which are efficient on a wide range of models and model representations, then many nascent modeling areas could accelerate their progress significantly. One early example of the types of applications these techniques could make possible is the modeling of natural language understanding by the mutually recursive simulation of a listener reasoning about a speaker who is in turn reasoning about his listener \cite{goodman2013knowledge, frank2012predicting}. 

\section{Approach and Roadmap}
In Chapter \ref{chap:perfComp} we perform an empirical comparison of the inference performance of Venture and OpenBUGS, which are two PPLs which make very different efficiency/expresiveness compromises. By testing different models on these engines we can gain a better idea of where the current systems are in most need of improvements, and how such improvements might be carried out. 

In Chapter\ref{chap:partPriors} we pursue a first possible area of improvement, by considering the feasibility of a pre-processing step which compiles a probabilistic program into an equivalent, but more efficient, form. As far as we are aware the idea for this pre-processing step is novel. The empirical results suggest that significant speed-ups are possible, but that this performance is dependant on the model being compiled. 

In \ref{chap:infEngines} we explore a second potential inference engine improvement, this time by implementing a slice sampling inference engine. In order to perform this investigation we create a novel probabilistic programming language, called Stochastic Python. Slice sampling has not been previously used as part of a Turing Complete PPL, and in our empirical comparison shows promising results on several models when compared to the default Metropolis-Hastings choice. Stochastic Python is made avaialble online \cite{stocPy}. 

Chapter \ref{chap:relWork} first does a very brief review of the types of PPLs currently in existence, so that our empirical comparison of Venture and OpenBUGS can be placed in context. We then look at work related to speeding up inference in PPLs, and specifically related to rewriting probabilistic programs and to implementing engines based on inference techniques other than Metropolis-Hastings. 

Finally, Chapter \ref{chap:conc} discusses the implications of the analysis done in earlier chapters and presents some possible avenues of future work.
